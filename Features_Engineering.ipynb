{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frikel12/Machine-Learning-and-Deep-Learningproject/blob/main/Features_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5enhfiSt2VLx"
      },
      "source": [
        "<center> <h1>Ingénierie des caractéristiques <br> (Features engineering)</h1></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA-qglwY03HC"
      },
      "source": [
        "<h1>Ingénierie de caractéristiques</h1>\n",
        "L'Ingénierie de caractéristiques consiste en le traitement des caractéristiques dans le but d'améliorer le comportement du modéle de classification. <br>\n",
        "Plusieurs étapes peuvent être utilisées (pas toutes utilisables pour l’image mining)\n",
        "\n",
        "* Valeurs manquantes (Non rencotrés en Image Mining)\n",
        "* Valeurs aberrantes\n",
        "* Transformation logarithmique\n",
        "* Encodage unique (Concerne uniquement les étiquettes en Image Mining)\n",
        "* Mise en échelle\n",
        "* Sélection des caractéristiques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6fblqPN1-VW"
      },
      "source": [
        "<h1> Implémentation </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7Maj0p2PHo"
      },
      "source": [
        "<h2> Préparation de l'environnement</h2>\n",
        "Nous allons travailler dans cet atelier sur des caractéristiques déja extraites et enregistrées sur Google Drive  (Voir atelier tranfer Learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az36GUAH63ZL",
        "outputId": "886d07fa-a780-454e-81d5-068a38412913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKCruVyWOig"
      },
      "source": [
        "<h2> Chargement de la dataset</h2>\n",
        "Pour tester et appliquer l'ingénierie de caractéristiques, nous allons utiliser des caractéristiques déjà extraites utilisant une architecture Deep Learning VGG16 de la base d'image Corel utilisée dans l'atelier de classification supervisée.\n",
        "Pour télécharger le fichier de caractéristiques et les étiquettes cliquer sur les liens ci-dessous :\n",
        "\n",
        "[Features](https://drive.google.com/file/d/105zEqcYD1Deuzy5NM5dOPqnXDETUAfVt/view?usp=share_link)\n",
        "\n",
        "\n",
        "[Labels](https://drive.google.com/file/d/1aggw9QXm9ao7CEmLa4ign9xi2zovUPnB/view?usp=drivesdk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDSBKCVrxuCp"
      },
      "source": [
        "<h3>Charger les caractéristiques et les étiquettes.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mocd_cxfmnF",
        "outputId": "576ad336-9ff0-4a1c-ad17-bc7b96fb4aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 1000) (490,)\n"
          ]
        }
      ],
      "source": [
        "# Load features and labels\n",
        "import pickle\n",
        "fetauresPath='/content/drive/MyDrive/Datasets/Image Mining/'\n",
        "X = pickle.load( open( fetauresPath+\"features_vgg16\", \"rb\" ) )\n",
        "y = pickle.load( open( fetauresPath+\"labels\", \"rb\" ) )\n",
        "\n",
        "import numpy as np\n",
        "print(np.array(X).shape, np.array(y).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQR4kVA93NJz"
      },
      "source": [
        "Pour chacune des étapes de l'ingénierie de caractéristiques, nous allons évaluer à chaque fois la classification afin de voir l'impact.\n",
        "Compléter l'implémentation de la fonction de classification. Utiliser un algorithme de classification de votre choix.\n",
        "La méthode doit afficher le taux de classification (accuracy) ainsi que le temps d'exécution de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8te-Q8eUcKZA"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "def classify(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  model = svm.SVC(kernel='rbf')\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13dK8hRi4PY0"
      },
      "source": [
        "Classification utilisant les caractéristiques initiales sans manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDSvx4pV4Oal",
        "outputId": "542f5e26-b774-4fd7-c964-57f0e9811bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.94\n"
          ]
        }
      ],
      "source": [
        "# Afficher l'accuracy\n",
        "print(\"accuracy : \", round(classify(X,y), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DM0ri2E4CUQ"
      },
      "source": [
        "<h1>Mise en échelle</h1>\n",
        "Dans la plupart des cas, les caractéristiques de type numérique ne sont pas dans la même plage d’intervalle.\n",
        "Exemple : les données sur les âges et les salaires différent et ne n’ont pas la même fourchette.\n",
        "Pour un algorithme d’apprentissage automatique, les valeurs qui ne changent pas beaucoup même sur différentes échelles vont être très influentes en termes de classification.\n",
        "Les algorithmes de classification basés sur le voisinage ou distance sont très sensible à la mise en échelle ; comme le KNN, K-means, …\n",
        "Pour mettre en échelle les données nous avons deux solutions :\n",
        "\n",
        "1.   Normalisation : min-max normalization\n",
        "2.   Standardisation : z-score normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMAS9qf7cAk"
      },
      "source": [
        "<h2> Standarization: z-score normalization</h2>\n",
        "<center>z=(X-µ)/σ\n",
        "\n",
        "X est l'ensemble d'apprentissage, µ est la moyenne et  σ est l’écart type\n",
        "</center>\n",
        "Le z-score met en échelle les valeurs en prenant en compte l’écart type (déviation standard). <br>\n",
        "le z-score réduit l’effet des données aberrantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmxT5qUn4AqY",
        "outputId": "e6085bd9-fe47-462c-b8fa-fa2fb3c13dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.95\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Standarization\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        "z = (x - u) / s\n",
        "where u is the mean of the training samples or zero if with_mean=False\n",
        "s is the standard deviation of the training samples or one if with_std=False.\n",
        "\"\"\"\n",
        "# Xzscore est la matrice de caractéristiques après standardisation\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Xzscore = (X - mean)/std\n",
        "#print(np.array(Xzscore).shape)\n",
        "print(\"accuracy : \", round(classify(Xzscore,y), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_TAfUF8fEm"
      },
      "source": [
        "<h2>Normalization: min-max normalization</h2>\n",
        "<center>Xnorm=(X-Xmin)/(Xmax -Xmin)</center>\n",
        "Permet de mettre en échelle toutes les valeurs des caractéristiques dans une plage fixe entre 0 et 1.<br>\n",
        "L'inconvénient de la normalization par min-max est qu'elle augmente les effets des valeurs aberrantes.<br>\n",
        "Ainsi, avant la normalisation, il est recommandé de traiter les valeurs aberrantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDmpKJT9TaU",
        "outputId": "4b2045e9-0bff-462b-f92b-6fb2a5737b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Noramlization\n",
        "Xnorm=(X-Xmin)/(Xmax -Xmin).\n",
        "\"\"\"\n",
        "# Xminmax est la matrice de caractéristiques après normalisation\n",
        "\n",
        "Xminmax=(X - X.min(axis=0))/(X.max(axis=0) - X.min(axis=0))\n",
        "\n",
        "#print(np.array(Xminmax).shape)\n",
        "print(\"accuracy : \", round(classify(Xminmax,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvSAQajiRpSD"
      },
      "source": [
        "<h2> Transformation logarithmique</h2>\n",
        "Pour éliminer les données aberrantes, une des techniques les plus utilisées est la transformation logarithmique.\n",
        "\n",
        "*\tTraiter des données biaisées et, après transformation, la distribution devient plus proche de la normale.\n",
        "*\tRéduire l’ordre de grandeur des données. Exemple : la différence de taille n’est pas de la même grandeur que la différence d'âges\n",
        "*\tRéduit aussi l'effet des valeurs aberrantes grâce à la normalisation des différences d'amplitude\n",
        "\n",
        "NB : il ne faut pas que les données soient négatives\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzaegQ7dS-b_",
        "outputId": "4e4928c3-9e6e-4231-b628-c89a85b3c1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Transformation logarithmique\n",
        "# Xlog est la matrice de caractéristiques après transformation logarithmique\n",
        "Xlog = np.log(X)\n",
        "# Classification utilisant les caractéristiques aprés tranformation logarithmique\n",
        "print(\"accuracy : \", round(classify(Xlog,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTnv_Uuqru_7"
      },
      "source": [
        "Par la suite nous allons procéder a la mise en échelle des données après transformation logarithmique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_tnub6sBz5a",
        "outputId": "dc5f2e34-90e5-4ce7-bf3c-08b4d9f82dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Normalization aprés TL\n",
        "\n",
        "Xminmax_Log = (Xlog - Xlog.min(axis=0))/(Xlog.max(axis=0) - Xlog.min(axis=0))\n",
        "print(\"accuracy : \", round(classify(Xminmax_Log,y), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GF77nX0Sz1V",
        "outputId": "23508c18-cff8-4ed0-c92b-a84ce77eac99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Standarization + TL\n",
        "mean = np.mean(Xlog, axis=0)\n",
        "std = np.std(Xlog, axis=0)\n",
        "Xzscore_Log = (Xlog - mean)/std\n",
        "print(\"accuracy : \", round(classify(Xzscore_Log,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzEa3D0hvgho"
      },
      "source": [
        "<h1>Sélection de caractéristiques (features selection)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnUqnVBhytSl"
      },
      "source": [
        "<h2> Suppression des caractéristiques à faible variance</h2>\n",
        "Suppression des caractéristiques à faible variance (Removing features with low variance) est une méthode de sélection (élimination) basée sur le filtrage.<br>\n",
        "On peut la considérer comme une méthode de nettoyage de caractéristique qui  élémine toutes les caractéristiques dont la variance n'atteint pas un certain seuil.<br>\n",
        "Par défaut, elle supprime toutes les caractéristiques à variance nulle, c'est-à-dire les caractéristiques qui ont la même valeur dans tous les échantillons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH0iwZjCy7X8",
        "outputId": "13c4fa98-9a89-478a-959a-82f40fc3be79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Nous allons utiliser les caractéristiques après normalisation Min-Max\n",
        "# Xvth est la matrice de caractéristiques après suppression des caractéristiques (après normalisation Min-Max)  à faible variance\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "vth = VarianceThreshold()\n",
        "Xvth = vth.fit_transform(Xminmax)\n",
        "print(\"accuracy : \", round(classify(Xvth,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWGA_pFzams"
      },
      "source": [
        "<h2>chi2</h2>\n",
        "C’est un algorithme de sélection des caractéristiques qui appartient à la famille des algorithmes de filtrage basé sur la statistique 𝜒2. Cette méthode mesure l’écart à l’indépendance entre une caractéristique et une classe. Elle commence par un niveau de signification élevé pour toutes les caractéristiques pour la discrétisation et chaque caractéristique est triée en fonction de ses valeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2jeNK2Izhlg",
        "outputId": "fe90eadd-f98d-49a1-8fc2-45d4c77e393f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.95\n"
          ]
        }
      ],
      "source": [
        "#Xchi2 est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant chi2\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "chi2 = SelectKBest(chi2, k=100)\n",
        "Xchi2 = chi2.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(Xchi2,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEsfySe00s_Y"
      },
      "source": [
        "<h2>Recursive Feature Elimination (RFE)</h2>\n",
        "C’est une méthode de cartographie basée sur l'idée à plusieurs reprises, construire un modèle et choisir le meilleur ou le pire performant. Cette méthode qui appartient aux méthodes de filtrage est souvent utilisée comme étape de prétraitement pour les méthodes intégrée (souvent avec l’algorithme de classification SVM) afin de la généraliser à des grandes masses de données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D2nZnux09jD",
        "outputId": "875851b8-e2a4-4dee-9c4e-6b27e0c7810b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Xrfe est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant RFE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator = LogisticRegression()\n",
        "\n",
        "rfe = RFE(estimator, n_features_to_select=100)\n",
        "Xrfel = rfe.fit_transform(Xminmax, y)\n",
        "\n",
        "print(\"accuracy : \", round(classify(Xrfel,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BygazAk9mo_C"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-IqzWEcjpFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a0c6a0-1274-4590-ac56-bde8e9cac52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Code pour calculer le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification\n",
        "rfe = RFE(estimator, n_features_to_select=82)\n",
        "Xrfel = rfe.fit_transform(Xminmax, y)\n",
        "\n",
        "print(\"accuracy : \", round(classify(Xrfel,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlfaMjkGhMNM"
      },
      "source": [
        "<h1>Relief</h1>\n",
        "Tente de déterminer le plus proche voisin d'un certain nombre d'échantillons sélectionnés au hasard à partir de l'ensemble de données. Pour chaque échantillon sélectionné, les valeurs des caractéristiques sont comparées à ceux des voisins les plus proches et les scores pour chaque caractéristique sont mis à jour. L'idée est d'estimer la qualité des attributs en fonction de la qualité de leurs valeurs et faire la distinction entre des échantillons proches les uns des autres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YxdVPziq8wD",
        "outputId": "55cbfa93-0e61-4baa-c259-40f9ad7cc3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skrebate\n",
            "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate) (3.2.0)\n",
            "Building wheels for collected packages: skrebate\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29253 sha256=a35e12c7c43914d5073af7e1ff441f9f06fbf15f4776eb5c0f630a4c3ea7c694\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/67/40/683074a684607162bd0e34dcf7ccdfcab5861c3b2a83286f3a\n",
            "Successfully built skrebate\n",
            "Installing collected packages: skrebate\n",
            "Successfully installed skrebate-0.62\n"
          ]
        }
      ],
      "source": [
        "#pour utiliser la technique de selection Relief, on peut se servir de la bibliotheque skrebate\n",
        "!pip install skrebate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOUjTkg5jjHv",
        "outputId": "ae0f89b9-f739-453c-9175-2cfa31e3c328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.96\n"
          ]
        }
      ],
      "source": [
        "# XRelief est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant Relief\n",
        "from skrebate import ReliefF\n",
        "\n",
        "relief = ReliefF(n_features_to_select=100)\n",
        "XRelief = relief.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(XRelief,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURQ4cvq7-s6"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xJR__tO7-s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1178e0d5-7f1b-4e9f-de52-53dbd5696752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.96\n"
          ]
        }
      ],
      "source": [
        "# Code pour calculer le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification\n",
        "relief = ReliefF(n_features_to_select=40)\n",
        "XRelief = relief.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(XRelief,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85yhbzTsfwDj"
      },
      "source": [
        "<h1>Réduction de la dimensionnalité </h1>\n",
        "La réduction de la dimensionnalité transforme les caractéristiques en une dimension inférieure. Elle peut être considérée comme étant une méthode de  Selection ou de création (extraction) de caractéristiques où nous dérivons des informations à partir de l’ensemble de caractéristiques de base pour construire un nouveau sous espace de caractéristiques. <br>\n",
        "Ls approches de réduction de dimensionnalité les plus connues sont: PCA (Principal Component Analysis; Analyse en Composantes Principales (ACP)) et ICA (Independent Component Analysis, Analyse en Composantes Independantes (ACI))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCqFtKhxl5v7"
      },
      "source": [
        "<h2> ACP </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u1-zsXzviSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd9c167-1c54-4243-bfe8-28f43123237b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 120)\n",
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Xpca est la matrice de caractéristiques après réduction de la dimensionalité utilisant l'ACP\n",
        "# Essayer de changer la taille de réduction de l'ACP afin d'obtenir le meilleur taux de classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=120)\n",
        "Xpca = pca.fit_transform(Xminmax)\n",
        "print(np.array(Xpca).shape)\n",
        "print(\"accuracy : \", round(classify(Xpca,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dehxuzuHl9Jo"
      },
      "source": [
        "<h2> ACI </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNw7JwD0mCoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d14ff5-919b-4ca3-a2c4-4bd731cc4e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 90)\n",
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Xica est la matrice de caractéristiques après réduction de la dimensionalité utilisant l'ACI\n",
        "# Essayer de changer la taille de réduction de l'ACI afin d'obtenir le meilleur taux de classification\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "ica = FastICA(n_components=90)\n",
        "Xica = ica.fit_transform(Xminmax)\n",
        "print(np.array(Xica).shape)\n",
        "print(\"accuracy : \", round(classify(Xica,y), 2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}