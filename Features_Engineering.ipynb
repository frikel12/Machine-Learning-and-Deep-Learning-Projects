{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frikel12/Machine-Learning-and-Deep-Learningproject/blob/main/Features_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5enhfiSt2VLx"
      },
      "source": [
        "<center> <h1>Ing√©nierie des caract√©ristiques <br> (Features engineering)</h1></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA-qglwY03HC"
      },
      "source": [
        "<h1>Ing√©nierie de caract√©ristiques</h1>\n",
        "L'Ing√©nierie de caract√©ristiques consiste en le traitement des caract√©ristiques dans le but d'am√©liorer le comportement du mod√©le de classification. <br>\n",
        "Plusieurs √©tapes peuvent √™tre utilis√©es (pas toutes utilisables pour l‚Äôimage mining)\n",
        "\n",
        "* Valeurs manquantes (Non rencotr√©s en Image Mining)\n",
        "* Valeurs aberrantes\n",
        "* Transformation logarithmique\n",
        "* Encodage unique (Concerne uniquement les √©tiquettes en Image Mining)\n",
        "* Mise en √©chelle\n",
        "* S√©lection des caract√©ristiques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6fblqPN1-VW"
      },
      "source": [
        "<h1> Impl√©mentation </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7Maj0p2PHo"
      },
      "source": [
        "<h2> Pr√©paration de l'environnement</h2>\n",
        "Nous allons travailler dans cet atelier sur des caract√©ristiques d√©ja extraites et enregistr√©es sur Google Drive  (Voir atelier tranfer Learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az36GUAH63ZL",
        "outputId": "886d07fa-a780-454e-81d5-068a38412913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKCruVyWOig"
      },
      "source": [
        "<h2> Chargement de la dataset</h2>\n",
        "Pour tester et appliquer l'ing√©nierie de caract√©ristiques, nous allons utiliser des caract√©ristiques d√©j√† extraites utilisant une architecture Deep Learning VGG16 de la base d'image Corel utilis√©e dans l'atelier de classification supervis√©e.\n",
        "Pour t√©l√©charger le fichier de caract√©ristiques et les √©tiquettes cliquer sur les liens ci-dessous :\n",
        "\n",
        "[Features](https://drive.google.com/file/d/105zEqcYD1Deuzy5NM5dOPqnXDETUAfVt/view?usp=share_link)\n",
        "\n",
        "\n",
        "[Labels](https://drive.google.com/file/d/1aggw9QXm9ao7CEmLa4ign9xi2zovUPnB/view?usp=drivesdk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDSBKCVrxuCp"
      },
      "source": [
        "<h3>Charger les caract√©ristiques et les √©tiquettes.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mocd_cxfmnF",
        "outputId": "576ad336-9ff0-4a1c-ad17-bc7b96fb4aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 1000) (490,)\n"
          ]
        }
      ],
      "source": [
        "# Load features and labels\n",
        "import pickle\n",
        "fetauresPath='/content/drive/MyDrive/Datasets/Image Mining/'\n",
        "X = pickle.load( open( fetauresPath+\"features_vgg16\", \"rb\" ) )\n",
        "y = pickle.load( open( fetauresPath+\"labels\", \"rb\" ) )\n",
        "\n",
        "import numpy as np\n",
        "print(np.array(X).shape, np.array(y).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQR4kVA93NJz"
      },
      "source": [
        "Pour chacune des √©tapes de l'ing√©nierie de caract√©ristiques, nous allons √©valuer √† chaque fois la classification afin de voir l'impact.\n",
        "Compl√©ter l'impl√©mentation de la fonction de classification. Utiliser un algorithme de classification de votre choix.\n",
        "La m√©thode doit afficher le taux de classification (accuracy) ainsi que le temps d'ex√©cution de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8te-Q8eUcKZA"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "def classify(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  model = svm.SVC(kernel='rbf')\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13dK8hRi4PY0"
      },
      "source": [
        "Classification utilisant les caract√©ristiques initiales sans manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDSvx4pV4Oal",
        "outputId": "542f5e26-b774-4fd7-c964-57f0e9811bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.94\n"
          ]
        }
      ],
      "source": [
        "# Afficher l'accuracy\n",
        "print(\"accuracy : \", round(classify(X,y), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DM0ri2E4CUQ"
      },
      "source": [
        "<h1>Mise en √©chelle</h1>\n",
        "Dans la plupart des cas, les caract√©ristiques de type num√©rique ne sont pas dans la m√™me plage d‚Äôintervalle.\n",
        "Exemple : les donn√©es sur les √¢ges et les salaires diff√©rent et ne n‚Äôont pas la m√™me fourchette.\n",
        "Pour un algorithme d‚Äôapprentissage automatique, les valeurs qui ne changent pas beaucoup m√™me sur diff√©rentes √©chelles vont √™tre tr√®s influentes en termes de classification.\n",
        "Les algorithmes de classification bas√©s sur le voisinage ou distance sont tr√®s sensible √† la mise en √©chelle ; comme le KNN, K-means, ‚Ä¶\n",
        "Pour mettre en √©chelle les donn√©es nous avons deux solutions :\n",
        "\n",
        "1.   Normalisation : min-max normalization\n",
        "2.   Standardisation : z-score normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMAS9qf7cAk"
      },
      "source": [
        "<h2> Standarization: z-score normalization</h2>\n",
        "<center>z=(X-¬µ)/œÉ\n",
        "\n",
        "X est l'ensemble d'apprentissage, ¬µ est la moyenne et  œÉ est l‚Äô√©cart type\n",
        "</center>\n",
        "Le z-score met en √©chelle les valeurs en prenant en compte l‚Äô√©cart type (d√©viation standard). <br>\n",
        "le z-score r√©duit l‚Äôeffet des donn√©es aberrantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmxT5qUn4AqY",
        "outputId": "e6085bd9-fe47-462c-b8fa-fa2fb3c13dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.95\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Standarization\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        "z = (x - u) / s\n",
        "where u is the mean of the training samples or zero if with_mean=False\n",
        "s is the standard deviation of the training samples or one if with_std=False.\n",
        "\"\"\"\n",
        "# Xzscore est la matrice de caract√©ristiques apr√®s standardisation\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "Xzscore = (X - mean)/std\n",
        "#print(np.array(Xzscore).shape)\n",
        "print(\"accuracy : \", round(classify(Xzscore,y), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_TAfUF8fEm"
      },
      "source": [
        "<h2>Normalization: min-max normalization</h2>\n",
        "<center>Xnorm=(X-Xmin)/(Xmax -Xmin)</center>\n",
        "Permet de mettre en √©chelle toutes les valeurs des caract√©ristiques dans une plage fixe entre 0 et 1.<br>\n",
        "L'inconv√©nient de la normalization par min-max est qu'elle augmente les effets des valeurs aberrantes.<br>\n",
        "Ainsi, avant la normalisation, il est recommand√© de traiter les valeurs aberrantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDmpKJT9TaU",
        "outputId": "4b2045e9-0bff-462b-f92b-6fb2a5737b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Noramlization\n",
        "Xnorm=(X-Xmin)/(Xmax -Xmin).\n",
        "\"\"\"\n",
        "# Xminmax est la matrice de caract√©ristiques apr√®s normalisation\n",
        "\n",
        "Xminmax=(X - X.min(axis=0))/(X.max(axis=0) - X.min(axis=0))\n",
        "\n",
        "#print(np.array(Xminmax).shape)\n",
        "print(\"accuracy : \", round(classify(Xminmax,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvSAQajiRpSD"
      },
      "source": [
        "<h2> Transformation logarithmique</h2>\n",
        "Pour √©liminer les donn√©es aberrantes, une des techniques les plus utilis√©es est la transformation logarithmique.\n",
        "\n",
        "*\tTraiter des donn√©es biais√©es et, apr√®s transformation, la distribution devient plus proche de la normale.\n",
        "*\tR√©duire l‚Äôordre de grandeur des donn√©es. Exemple : la diff√©rence de taille n‚Äôest pas de la m√™me grandeur que la diff√©rence d'√¢ges\n",
        "*\tR√©duit aussi l'effet des valeurs aberrantes gr√¢ce √† la normalisation des diff√©rences d'amplitude\n",
        "\n",
        "NB : il ne faut pas que les donn√©es soient n√©gatives\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzaegQ7dS-b_",
        "outputId": "4e4928c3-9e6e-4231-b628-c89a85b3c1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Transformation logarithmique\n",
        "# Xlog est la matrice de caract√©ristiques apr√®s transformation logarithmique\n",
        "Xlog = np.log(X)\n",
        "# Classification utilisant les caract√©ristiques apr√©s tranformation logarithmique\n",
        "print(\"accuracy : \", round(classify(Xlog,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTnv_Uuqru_7"
      },
      "source": [
        "Par la suite nous allons proc√©der a la mise en √©chelle des donn√©es apr√®s transformation logarithmique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_tnub6sBz5a",
        "outputId": "dc5f2e34-90e5-4ce7-bf3c-08b4d9f82dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Normalization apr√©s TL\n",
        "\n",
        "Xminmax_Log = (Xlog - Xlog.min(axis=0))/(Xlog.max(axis=0) - Xlog.min(axis=0))\n",
        "print(\"accuracy : \", round(classify(Xminmax_Log,y), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GF77nX0Sz1V",
        "outputId": "23508c18-cff8-4ed0-c92b-a84ce77eac99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Standarization + TL\n",
        "mean = np.mean(Xlog, axis=0)\n",
        "std = np.std(Xlog, axis=0)\n",
        "Xzscore_Log = (Xlog - mean)/std\n",
        "print(\"accuracy : \", round(classify(Xzscore_Log,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzEa3D0hvgho"
      },
      "source": [
        "<h1>S√©lection de caract√©ristiques (features selection)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnUqnVBhytSl"
      },
      "source": [
        "<h2> Suppression des caract√©ristiques √† faible variance</h2>\n",
        "Suppression des caract√©ristiques √† faible variance (Removing features with low variance) est une m√©thode de s√©lection (√©limination) bas√©e sur le filtrage.<br>\n",
        "On peut la consid√©rer comme une m√©thode de nettoyage de caract√©ristique qui  √©l√©mine toutes les caract√©ristiques dont la variance n'atteint pas un certain seuil.<br>\n",
        "Par d√©faut, elle supprime toutes les caract√©ristiques √† variance nulle, c'est-√†-dire les caract√©ristiques qui ont la m√™me valeur dans tous les √©chantillons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH0iwZjCy7X8",
        "outputId": "13c4fa98-9a89-478a-959a-82f40fc3be79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Nous allons utiliser les caract√©ristiques apr√®s normalisation Min-Max\n",
        "# Xvth est la matrice de caract√©ristiques apr√®s suppression des caract√©ristiques (apr√®s normalisation Min-Max)  √† faible variance\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "vth = VarianceThreshold()\n",
        "Xvth = vth.fit_transform(Xminmax)\n",
        "print(\"accuracy : \", round(classify(Xvth,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWGA_pFzams"
      },
      "source": [
        "<h2>chi2</h2>\n",
        "C‚Äôest un algorithme de s√©lection des caract√©ristiques qui appartient √† la famille des algorithmes de filtrage bas√© sur la statistique ùúí2. Cette m√©thode mesure l‚Äô√©cart √† l‚Äôind√©pendance entre une caract√©ristique et une classe. Elle commence par un niveau de signification √©lev√© pour toutes les caract√©ristiques pour la discr√©tisation et chaque caract√©ristique est tri√©e en fonction de ses valeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2jeNK2Izhlg",
        "outputId": "fe90eadd-f98d-49a1-8fc2-45d4c77e393f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.95\n"
          ]
        }
      ],
      "source": [
        "#Xchi2 est la matrice de caract√©ristiques apr√®s s√©lection de 100 caract√©ristiques utilisant chi2\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "chi2 = SelectKBest(chi2, k=100)\n",
        "Xchi2 = chi2.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(Xchi2,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEsfySe00s_Y"
      },
      "source": [
        "<h2>Recursive Feature Elimination (RFE)</h2>\n",
        "C‚Äôest une m√©thode de cartographie bas√©e sur l'id√©e √† plusieurs reprises, construire un mod√®le et choisir le meilleur ou le pire performant. Cette m√©thode qui appartient aux m√©thodes de filtrage est souvent utilis√©e comme √©tape de pr√©traitement pour les m√©thodes int√©gr√©e (souvent avec l‚Äôalgorithme de classification SVM) afin de la g√©n√©raliser √† des grandes masses de donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D2nZnux09jD",
        "outputId": "875851b8-e2a4-4dee-9c4e-6b27e0c7810b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Xrfe est la matrice de caract√©ristiques apr√®s s√©lection de 100 caract√©ristiques utilisant RFE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimator = LogisticRegression()\n",
        "\n",
        "rfe = RFE(estimator, n_features_to_select=100)\n",
        "Xrfel = rfe.fit_transform(Xminmax, y)\n",
        "\n",
        "print(\"accuracy : \", round(classify(Xrfel,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BygazAk9mo_C"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caract√©ristiques √† utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-IqzWEcjpFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a0c6a0-1274-4590-ac56-bde8e9cac52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.98\n"
          ]
        }
      ],
      "source": [
        "# Code pour calculer le nombre minimal de caract√©ristiques √† utiliser pour obtenir le taux maximal de classification\n",
        "rfe = RFE(estimator, n_features_to_select=82)\n",
        "Xrfel = rfe.fit_transform(Xminmax, y)\n",
        "\n",
        "print(\"accuracy : \", round(classify(Xrfel,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlfaMjkGhMNM"
      },
      "source": [
        "<h1>Relief</h1>\n",
        "Tente de d√©terminer le plus proche voisin d'un certain nombre d'√©chantillons s√©lectionn√©s au hasard √† partir de l'ensemble de donn√©es. Pour chaque √©chantillon s√©lectionn√©, les valeurs des caract√©ristiques sont compar√©es √† ceux des voisins les plus proches et les scores pour chaque caract√©ristique sont mis √† jour. L'id√©e est d'estimer la qualit√© des attributs en fonction de la qualit√© de leurs valeurs et faire la distinction entre des √©chantillons proches les uns des autres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YxdVPziq8wD",
        "outputId": "55cbfa93-0e61-4baa-c259-40f9ad7cc3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skrebate\n",
            "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from skrebate) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate) (3.2.0)\n",
            "Building wheels for collected packages: skrebate\n",
            "  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29253 sha256=a35e12c7c43914d5073af7e1ff441f9f06fbf15f4776eb5c0f630a4c3ea7c694\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/67/40/683074a684607162bd0e34dcf7ccdfcab5861c3b2a83286f3a\n",
            "Successfully built skrebate\n",
            "Installing collected packages: skrebate\n",
            "Successfully installed skrebate-0.62\n"
          ]
        }
      ],
      "source": [
        "#pour utiliser la technique de selection Relief, on peut se servir de la bibliotheque skrebate\n",
        "!pip install skrebate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOUjTkg5jjHv",
        "outputId": "ae0f89b9-f739-453c-9175-2cfa31e3c328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.96\n"
          ]
        }
      ],
      "source": [
        "# XRelief est la matrice de caract√©ristiques apr√®s s√©lection de 100 caract√©ristiques utilisant Relief\n",
        "from skrebate import ReliefF\n",
        "\n",
        "relief = ReliefF(n_features_to_select=100)\n",
        "XRelief = relief.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(XRelief,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURQ4cvq7-s6"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caract√©ristiques √† utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xJR__tO7-s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1178e0d5-7f1b-4e9f-de52-53dbd5696752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy :  0.96\n"
          ]
        }
      ],
      "source": [
        "# Code pour calculer le nombre minimal de caract√©ristiques √† utiliser pour obtenir le taux maximal de classification\n",
        "relief = ReliefF(n_features_to_select=40)\n",
        "XRelief = relief.fit_transform(Xminmax, y)\n",
        "print(\"accuracy : \", round(classify(XRelief,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85yhbzTsfwDj"
      },
      "source": [
        "<h1>R√©duction de la dimensionnalit√© </h1>\n",
        "La r√©duction de la dimensionnalit√© transforme les caract√©ristiques en une dimension inf√©rieure. Elle peut √™tre consid√©r√©e comme √©tant une m√©thode de  Selection ou de cr√©ation (extraction) de caract√©ristiques o√π nous d√©rivons des informations √† partir de l‚Äôensemble de caract√©ristiques de base pour construire un nouveau sous espace de caract√©ristiques. <br>\n",
        "Ls approches de r√©duction de dimensionnalit√© les plus connues sont: PCA (Principal Component Analysis; Analyse en Composantes Principales (ACP)) et ICA (Independent Component Analysis, Analyse en Composantes Independantes (ACI))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCqFtKhxl5v7"
      },
      "source": [
        "<h2> ACP </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u1-zsXzviSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd9c167-1c54-4243-bfe8-28f43123237b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 120)\n",
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Xpca est la matrice de caract√©ristiques apr√®s r√©duction de la dimensionalit√© utilisant l'ACP\n",
        "# Essayer de changer la taille de r√©duction de l'ACP afin d'obtenir le meilleur taux de classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=120)\n",
        "Xpca = pca.fit_transform(Xminmax)\n",
        "print(np.array(Xpca).shape)\n",
        "print(\"accuracy : \", round(classify(Xpca,y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dehxuzuHl9Jo"
      },
      "source": [
        "<h2> ACI </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNw7JwD0mCoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d14ff5-919b-4ca3-a2c4-4bd731cc4e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 90)\n",
            "accuracy :  0.97\n"
          ]
        }
      ],
      "source": [
        "# Xica est la matrice de caract√©ristiques apr√®s r√©duction de la dimensionalit√© utilisant l'ACI\n",
        "# Essayer de changer la taille de r√©duction de l'ACI afin d'obtenir le meilleur taux de classification\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "ica = FastICA(n_components=90)\n",
        "Xica = ica.fit_transform(Xminmax)\n",
        "print(np.array(Xica).shape)\n",
        "print(\"accuracy : \", round(classify(Xica,y), 2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}